{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKU9qV76KpTr"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style= \"darkgrid\", color_codes = True)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Conv2DTranspose, Reshape, BatchNormalization, Dropout, Input, ReLU, LeakyReLU\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from PIL import Image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBFDQOVbK_ch",
        "outputId": "313efa28-a25c-4654-afb5-180180830f01"
      },
      "outputs": [],
      "source": [
        "# Loading and Preparing Anime Face Images Dataset using Keras Image Data Generator\n",
        "img_width, img_height = 256, 256\n",
        "batchsize = 32\n",
        "\n",
        "train = keras. utils.image_dataset_from_directory(\n",
        "    directory='/home/devanpatel/workspace/Workspace/dataset2',\n",
        "    batch_size = batchsize,\n",
        "    image_size = (img_width, img_height))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "gxZylE04Q5IK",
        "outputId": "e8cab9ee-ebf1-451a-c6d7-a1feb6b2bc94"
      },
      "outputs": [],
      "source": [
        "# Visualizing a Batch of Anime Face Images\n",
        "\n",
        "data_iterator = train.as_numpy_iterator()\n",
        "batch = data_iterator.next()\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(10,10))\n",
        "for idx, img in enumerate(batch[0][:4]):\n",
        "    ax[idx].imshow(img.astype(int))\n",
        "    ax[idx].title.set_text(batch[1][idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqTt2z_KRurr",
        "outputId": "d1b716be-52b2-4c75-ee19-0822184b14ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 63565 images belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "# Generating Augmented Batches of Anime Face Images using ImageDataGenerator\n",
        "DIR = '/home/devanpatel/workspace/Workspace/dataset2' #path\n",
        "\n",
        "# Create an ImageDataGenerator object with data augmentation options for image preprocessing\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        DIR,\n",
        "        target_size = (64, 64),\n",
        "        batch_size = batchsize,\n",
        "        class_mode = None)\n",
        "\n",
        "#train_generator[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ao0W8uB_R-7A",
        "outputId": "d17e6d08-c85d-4d2c-c3d0-b11e510c5e35"
      },
      "outputs": [],
      "source": [
        "# Creating the Generator Model\n",
        "\n",
        "KI = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "input_dim = 100\n",
        "\n",
        "def Generator_Model():\n",
        "    Generator = Sequential()\n",
        "\n",
        "    # Random noise\n",
        "    Generator.add(Dense(8 * 8 * 512, input_dim=input_dim))\n",
        "    Generator.add(ReLU())\n",
        "    # Convert 1d to 3d\n",
        "    Generator.add(Reshape((8, 8, 512)))\n",
        "    # Unsample\n",
        "    Generator.add(Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=KI))\n",
        "    Generator.add(ReLU())  # Adding ReLU activation function separately\n",
        "    Generator.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=KI))\n",
        "    Generator.add(ReLU())  # Adding ReLU activation function separately\n",
        "    Generator.add(Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=KI))\n",
        "    Generator.add(ReLU())  # Adding ReLU activation function separately\n",
        "    Generator.add(Conv2D(3, (4, 4), padding='same', activation='sigmoid'))\n",
        "\n",
        "    return Generator\n",
        "\n",
        "\n",
        "generator = Generator_Model()\n",
        "generator.summary()\n",
        "# Visualized Layers of generator\n",
        "keras.utils.plot_model(generator, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9oivBnZzSJsQ",
        "outputId": "59f5f7c2-7e92-4c35-bcb2-18b478ea4e98"
      },
      "outputs": [],
      "source": [
        "# Creating the discriminator Model\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "def Discriminator_Model():\n",
        "    input_shape = (64, 64, 3)\n",
        "\n",
        "    # Create a Sequential model\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Conv2D(64, kernel_size=(3, 3), input_shape=input_shape))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))  # Adding LeakyReLU activation function\n",
        "    discriminator.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    discriminator.add(Conv2D(128, kernel_size=(3, 3)))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))  # Adding LeakyReLU activation function\n",
        "    discriminator.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    discriminator.add(Conv2D(256, kernel_size=(3, 3)))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))  # Adding LeakyReLU activation function\n",
        "    discriminator.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    discriminator.add(Flatten())\n",
        "    discriminator.add(Dense(256))\n",
        "    discriminator.add(LeakyReLU(alpha=0.2))  # Adding LeakyReLU activation function\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return discriminator\n",
        "\n",
        "discriminator = Discriminator_Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7fJS7r7HSSbM"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DCGAN(keras.Model):\n",
        "    def __init__(self, generator, discriminator, latent_dim = input_dim):\n",
        "        super().__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.g_loss_metric = keras.metrics.Mean(name='g_loss')\n",
        "        self.d_loss_metric = keras.metrics.Mean(name='d_loss')\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.g_loss_metric, self.d_loss_metric]\n",
        "\n",
        "    def compile(self, g_optimizer, d_optimizer, loss_fn):\n",
        "        super(DCGAN, self).compile()\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_noise = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            pred_real = self.discriminator(real_images, training=True)\n",
        "            real_labels = tf.ones((batch_size, 1))\n",
        "            real_labels += 0.05 * tf.random.uniform(tf.shape(real_labels))\n",
        "            d_loss_real = self.loss_fn(real_labels, pred_real)\n",
        "\n",
        "            fake_images = self.generator(random_noise)\n",
        "            pred_fake = self.discriminator(fake_images, training=True)\n",
        "            fake_labels = tf.zeros((batch_size, 1))\n",
        "            d_loss_fake = self.loss_fn(fake_labels, pred_fake)\n",
        "\n",
        "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
        "\n",
        "        gradients = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_optimizer.apply_gradients(zip(gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "\n",
        "        labels = tf.ones((batch_size, 1))\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_images = self.generator(random_noise, training=True)\n",
        "            pred_fake = self.discriminator(fake_images, training=True)\n",
        "            g_loss = self.loss_fn(labels, pred_fake)\n",
        "\n",
        "        gradients = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        self.g_optimizer.apply_gradients(zip(gradients, self.generator.trainable_variables))\n",
        "\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "\n",
        "        return {'d_loss': self.d_loss_metric.result(), 'g_loss': self.g_loss_metric.result()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DCGAN Monitor for Image Generation and Model Saving\n",
        "\n",
        "class DCGANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_imgs=25, latent_dim = input_dim):\n",
        "        self.num_imgs = num_imgs\n",
        "        self.latent_dim = latent_dim\n",
        "        # create random noise for generating images\n",
        "        self.noise = tf.random.normal([25, latent_dim])\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs = None):\n",
        "        # generate the image from noise\n",
        "        g_img = self.model.generator(self.noise)\n",
        "        # denormalize the image\n",
        "        g_img = (g_img * 255) + 255\n",
        "        g_img.numpy()\n",
        "\n",
        "    def on_train_end(self, logs = None):\n",
        "        self.model.generator.save('DCGEN3.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v6HiFqiqSbzF",
        "outputId": "943ee4d7-cbc8-4d6e-b7d9-6976a438bfff"
      },
      "outputs": [],
      "source": [
        "# Training DCGAN on Image Dataset for 40 Epochs\n",
        "\n",
        "epochs = 50\n",
        "lr_g =0.0003\n",
        "lr_d = 0.0001\n",
        "beta = 0.5\n",
        "latent_dim = 250\n",
        "\n",
        "dcgan = DCGAN(generator=generator, discriminator=discriminator, latent_dim = latent_dim )\n",
        "dcgan.compile(g_optimizer = Adam (learning_rate= lr_g, beta_1= beta), d_optimizer= Adam (learning_rate = lr_g , beta_1= beta), loss_fn = BinaryCrossentropy())\n",
        "\n",
        "# Fit the model and save the history\n",
        "history = dcgan.fit(train_generator, epochs=epochs, callbacks=[DCGANMonitor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generating 36 Random Images with DCGAN\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "\n",
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    # Generate random noise for each image\n",
        "    noise = tf.random.normal([1, 250])\n",
        "    mg = dcgan.generator(noise)\n",
        "    # Denormalize\n",
        "    mg = (mg * 255)\n",
        "\n",
        "    mg.numpy()\n",
        "    image = Image.fromarray(np.uint8(mg[0]))\n",
        "\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
